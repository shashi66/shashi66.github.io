<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shashi Kumar</title>
  
  <meta name="author" content="Shashi Kumar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shashi Kumar</name>
              </p>
              <p>I am a Lead ML Engineer at <a href="https://thelevel.ai/">Level AI</a>, where I work on Automatic Speech Recognition (ASR) and Machine Learning.
              </p>
              <p>
                Previously, I was at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute India - Bangalore</a>, where I worked on ASR, Speech Enhancement and Speaker Adaptation under the supervision of <a href="https://www.linkedin.com/in/shakti-p-rath-669b5118/">Dr. Shakti P. Rath</a>.
                I also participated in <a href="https://dicova2021.github.io/">DiCOVA 2021</a> challenge organized by <a href="https://www.interspeech2021.org/">Interspeech 2021</a> where the aim was to detect COVID-19 using cough sounds. On final <a href="https://competitions.codalab.org/competitions/29640#results">leaderboard</a>, our team was ranked 5th out of 29 teams.
              </p>
              <p>
                I finished my undergrad from Indian Institute of Technology (IIT), Guwahati with a major in <a href="https://www.iitg.ac.in/eee/">Electronics and Communication Engineering (ECE)</a>.
                I worked on <a href="pdfs/Aesthetic_Rating_and_Emotion_Analysis_using_deep_learning.pdf">Image Aesthetics and Emotion Analysis in Natural Images</a> for my Bachelor Thesis Project under the supervision of <a href="https://www.ee.iitb.ac.in/~asethi/">Prof. Amit Sethi</a>.
                In the Second year, I interned at <a href="http://www.cvl.cs.chubu.ac.jp/">Chubu University, Japan</a> under the supervision of <a href="https://scholar.google.ca/citations?user=zJZqXJsAAAAJ&hl=en">Prof. Yuji Iwahori</a>.
                I have also worked on <a href="pdfs/resume_shashi.pdf">multiple problems</a> in NLP domain.
              </p>
              <p style="text-align:center">
                <a href="mailto:shashiiitg5572@gmail.com">Email</a> &nbsp/&nbsp
                <a href="pdfs/resume_shashi.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?hl=en&pli=1&user=H7NGbnYAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_pic_2_crop.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic_2_crop.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am deeply interested in research around Machine Learning and its applications in multiple domains. At present, I am working on multiple things like improving factorization of latent spaces for generative models, 
                improving ASR performance under different conditions, modelling conversational text into DAGs for better understanding.
                
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 	

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/matched_joint_vae.png" alt="joint-vae-ff" width="180" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2204.11286" id="paper_link_jvae_ff">
                <papertitle>Improved far-field speech recognition using Joint Variational Autoencoder</papertitle>
              </a>
              <br>
              <strong>Shashi Kumar</strong>,  <a href="https://www.linkedin.com/in/shakti-p-rath-669b5118/">Shakti P. Rath</a>, <a href="https://www.linkedin.com/in/abhishek-pandey-03/">Abhishek Pandey</a>
              <br>
              <em>arXiv, 2022</em>
              <br>
               <a href="https://arxiv.org/pdf/2204.11286.pdf">pdf</a>
              <p>We propose joint training of acoustic model (AM) with joint VAE based speech enhancement. Approximations made in original joint VAE formulation has been relaxed and their effects on Word Error Rate (WER) has been analyzed.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/speaker_norm_paper_jvae.png" alt="joint-vae-speaker" width="180" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2021/kumar21b_interspeech.html" id="paper_link_jvae_speaker">
                <papertitle>Speaker Normalization Using Joint Variational Autoencoder</papertitle>
              </a>
              <br>
              <strong>Shashi Kumar</strong>,  <a href="https://www.linkedin.com/in/shakti-p-rath-669b5118/">Shakti P. Rath</a>, <a href="https://www.linkedin.com/in/abhishek-pandey-03/">Abhishek Pandey</a>
              <br>
              <em>Interspeech, 2021</em>
              <br>
               <a href="https://www.youtube.com/watch?v=5oDYSiF1PC4">YouTube Video</a> / <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/kumar21b_interspeech.pdf">pdf</a>
              <p>We propose to map Speaker Independent (SI) features to Speaker Normalized (SN) space. CMLLR normalized space is chosen as SN space. Additionally, we achieve WER similar as Speaker Adaptive Training (SAT) methods in a single pass decoding.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/whisper_paper_Interspeech.png" alt="joint-vae-whisper" width="180" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2021/agrawal21_interspeech.html" id="paper_link_jvae_whisper">
                <papertitle>Whisper Speech Enhancement Using Joint Variational Autoencoder for Improved Speech Recognition</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/vikas-agrawal-b0b44a69/">Vikas Agrawal</a>, <strong>Shashi Kumar</strong>,  <a href="https://www.linkedin.com/in/shakti-p-rath-669b5118/">Shakti P. Rath</a>
              <br>
              <em>Interspeech, 2021</em>
              <br>
               <a href="https://www.youtube.com/watch?v=nOF2C44RQ_g">YouTube Video</a> / <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/agrawal21_interspeech.pdf">pdf</a>
              <p>To counter lack of formants etc in whisper speech, We propose to map whisper speech to normal speech and train AM jointly with this enhancment model. We show a significant improvement in WER.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dicova_2021.png" alt="dicova2021" width="180" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.07972" id="dicova_2021">
                <papertitle>SRIB Submission to Interspeech 2021 DiCOVA Challenge</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/vishwanath-pratap-singh-436643ab/">Vishwanath Pratap Singh*</a>, <strong>Shashi Kumar*</strong>,  <a href="https://www.linkedin.com/in/ravi-shekhar-jha/"> Ravi Shekhar Jha*</a>, <a href="https://www.linkedin.com/in/abhishek-pandey-03/">Abhishek Pandey</a>
              <br>
              <em>arXiv, 2021</em>
              <br>
               <a href="https://dicova2021.github.io/">Challenge Link</a> / <a href="https://competitions.codalab.org/competitions/29640">Leaderboard</a>
              <p>The main aim of this challenge was to detect covid-19 using cough sounds. Our submission which used an ensemble of multiple models alongwith segment and frame level handcrafted features achieved 5th rank out of 29 teams.</p>
            </td>
          </tr>

          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hetero_loss_mle.png" alt="hetero-enc" width="180" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2019/kumar19_interspeech.html" id="paper_link_hetero_enc">
                <papertitle>Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition</papertitle>
              </a>
              <br>
              <strong>Shashi Kumar</strong>,  <a href="https://www.linkedin.com/in/shakti-p-rath-669b5118/">Shakti P. Rath</a>
              <br>
              <em>Interspeech, 2019</em>
              <br>
               <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2019/kumar19_interspeech.pdf">pdf</a>
              <p>We propose a more generalized loss based on non-zero mean and heteroscedastic co-variance distribution for the residual variables in regression MLE estimates. We also propose suitable architectures for the final loss. Overall, we show a significant improvement in WER on AMI SDM set.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/joint-vae-orig.png" alt="joint-vae-orig" width="180" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9004024" id="paper_link_jvae">
                <papertitle>Joint Distribution Learning in the Framework of Variational Autoencoders for Far-Field Speech Enhancement</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/chelimillamaheshkumar/">Mahesh K. Chelimilla</a>, <strong>Shashi Kumar</strong>,  <a href="https://www.linkedin.com/in/shakti-p-rath-669b5118/">Shakti P. Rath</a>
              <br>
              <em>ASRU, 2019</em>
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/9004024">link</a>
              <p>We propose novel modifications in the conventional VAE to model joint distribution of the far-field and close-talk features for a common latent space. We show a significant improvement in WER on AMI SDM set.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pcb_def_paper.png" alt="pcb-def" width="160" height="150" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-981-10-2104-6_47" id="paper_link_pcb_def">
                <papertitle>PCB Defect Classification Using Logical Combination of Segmented Copper and Non-copper Part</papertitle>
              </a>
              <br>
              <strong>Shashi Kumar</strong>,  <a href="http://www.cvl.cs.chubu.ac.jp/">Yuji Iwahori</a>, <a href="https://iitg.ac.in/mkb/index.php">M. K. Bhuyan </a>
              <br>
              <em>CVIP, 2017</em>
              <br>
               <a href="https://www.researchgate.net/profile/Yuji-Iwahori/publication/311857243_PCB_Defect_Classification_Using_Logical_Combination_of_Segmented_Copper_and_Non-copper_Part/links/5a772a2daca2722e4df0fd3d/PCB-Defect-Classification-Using-Logical-Combination-of-Segmented-Copper-and-Non-copper-Part.pdf">pdf</a>
              <p>We propose handcrafted features to classify defects in PCBs. The proposed approach is deployed in actual Industrial use.</p>
            </td>
          </tr>
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                Huge thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>